
<html>
<head>
<title> CS585 Homework : HW3 Student Name [Your Name Here 1]  </title>
<style>

body{
font-family: 'Trebuchet MS', Verdana;
}
p{
font-family: 'Trebuchet MS', Times;
margin: 10px 10px 15px 20px;
}
h3{
margin: 5px;
}
h2{
margin: 10px;
}
h1{
margin: 10px 0px 0px 20px;
}
div.main-body{
align:center;
margin: 30px;
}
hr{
margin:20px 0px 20px 0px;
}

.img{
  height: 260px;
}

.res{
  height: 180px;
}

.c{
   width: 500px;
   border: 1px solid black;
   vertical-align: middle;
   text-align: center;
}

td {

}
</style>
</head>

<body>
<center>
<a href="http://www.bu.edu"><img border="0" src="http://www.cs.bu.edu/fac/betke/images/bu-logo.gif"
width="119" height="120"></a>
</center>

<h1>Assignment 3</h1>
<p> 
 CS 585 HW 3 <br>
 [Your Name Here 2] <br>
 Teammate: Shijie Zhao, Jamie Nelson <br>
 Oct. 10. 2018
</p>

<div class="main-body">
<hr>
<h2> Problem Definition </h2>
<p>
<!--
Give a concise description of current problem.  What
needs to be solved?  Why is the result useful?  Do you make any assumptions?
What are the anticipated difficulties?
-->
    The main purpose of this assignment is to implement an algorithm that can recognize different hand shapes and gestures from a webcam (real-time) video as well as provide a visual representation of those results. More precisely, the tasks are to:<br/>

    <ol>
      <li> Read and display each video frame </li>
      <li> Segment the critical parts of the frame for the recognition process </li>
      <li> Use the template matching algorithm to determine which hand shapes are in the frame </li>
      <li> Track the gestures using multiple frames</li>
      <li> Create a visual representation of the results </li>
    </ol>
</p>

<hr>
<h2> Method and Implementation </h2>
<p>
<!--
  Give a concise description of the implemented method. For example, you might
  describe the motivation of your idea, the algorithmic steps of your methods, or
  the mathematical formulation of your method. 
-->
  <ol>
    <li>Template generation</li> <p>
    To create template images, we took photos of different hand shapes using both the left and right hand. For all of these images, we used a white background. These photos included hands with just the index finger pointing up, a hand in a closed fist, a palm with closed fingers, a palm with spread fingers, a hand with the thumb up, and a hand with the thumb down. Then, we used the same skin color detection algorithm from lab 3 to extract the hand in each image. We removed noise by thresholding and only using the area inside the largest contour. The hand template image is converted to grayscale and blurred for a more generic hand shape. </p>

    <li>Pre-processing</li>
    <p>
    The pre-processing of each frame is very similar to how we generated the template images. First, we used skin color detection to extract any area in the frame with a hand, leaving other sections of the frame black. Then, we used the mathematical morphology technique called opening to remove noise with skin color. We used an elliptical kernel so the image after the opening process remains "curvy" and similar to an actual hand shape. For the bigger blobs, we used thresholding to find the binary representations. The area of the blobs is calculated using the contour of the blobs. The area of a blob needs to meet a certain threshold to be identified as a hand. In this project, the area threshold is set to be 8000 pixels. Next, the detected blobs bigger than the threshold are passed to template matching.
    </p> 
    <li>Template matching</li>
    <p>
      Once we have obtained the larger skin-color blobs, we used the template matching algorithm to check whether each blob is a hand shape or not. 
    </p>
    <p>
      First, we resized all the template images to the blob size. By resizing the templates, we can ignore the bias in NCC and SSD values caused by the different image matching sizes. We chose to resize the image rather than using the image pyramid because resizing is more efficient (one match per image) and more effective (we do not need to worry about the proper image scale ratios). 
    </p>
    <p>
      Second, we compared the normalized correlation coefficient (NCC) values of the grayscale images. One benefit for the NCC is that it's normalized in the scale of [-1,1]. It is easy to set a threshold on multiple blobs to judge whether they should be accepted as a hand shape or not, no matter how many hands we have on the screen. Moreover, compared to computing the sum of squared differences (SSD) values on the binarized images, the NNC on grayscale images contains more information about the hand shapes, which can have better performance on the recognition job. 
    </p>
    <p>
      Furthermore, with the consideration of slight differences in angles among blobs and templates, we also adopted image rotations before the matching process. We created rotated blob images and compared them with the template images in order to choose the best-matched values.
    </p>
    <li>Movement detection</li>
    <p>
      For this part, two methods are considered. 
      <ol>
        <li>Motion energy template matching</li>
        <p>
          We created a union of binary difference images over the area of a waving hand, and used the area as a subimage. For the real time image input, we keep track of a list of image binaries, using the union of the binary of real time images, to match the template of a waving hand. However, due to the performance of the project from template matching, this method is eventually discarded.
        </p>
        <li>Object center tracking</li>
        <p>
          For each object found, we kept track of the center of the bounding box. For a sequence of images, we try to match object in the next image with the object in the previous frame by their Euclidean distance. The threshold is set to be 10000 pixels. If the object in the new frame is within 10000 pixels from another object from the previous image, we assume that they are the same object. We track a sequence of the center coordinates of each object in the image.
          <ol>
            <li>Waving</li>
            <p>
              From the sequence of center points for an object, the mean value of x coordinate is calculated and the average of absolute error between the mean and the x coordinates are calculated. If the average error is within a lower bound and an upper bound, the object is considered waving. If the average error is smaller than the lower bound, it is considered the natural movement of the body.
            </p>
            <li>Drawing</li>
            <p>The hand object is considered drawing, if the average error is above the waving upper bound.</p>
          </ol>
        </p>
      </ol>        
    </p>

    <li>Visual representation</li>
    <p>
      For areas detected to be a hand, the program will draw the bounding box of the object, along with the classification label on top or bottom of the box.
    </p>
    <p>
      For waving gestures, the program will attach the "waving" label on the blobs on the screen. When there are drawing movements, we use the trace queue that we stored previously to draw the paths of the movement, so that it's easy to see what has been drawn.
    </p>
    <li>Optimization</li>
    <p>The program has several bottlenecks. The most significant one being template matching. For this reason, we changed the implementation of moment energy template matching to simple center tracking. The result is better than we expected.</p>
    <p>
      Other optimizations involve vectorization of some functions. For example, vectorization of NCC function decreased the template matching function by around 10%. Also, the code has been formatted for better performance.
    </p>
  </ol>
</p>

<p>

  <!--
  Briefly outline the functions you created in your code to carry out the
  algorithmic steps you described earlier.-->
functions created:<br /><br />

<code>imgMax(img)</code>: Takes in an image, return a matrix of maximum values from a 3 channel image.<br />

<code>imgMin(img)</code>: Takes in an image, return a matrix of minimum values from a 3 channel image.<br />

<code>def skinDetect(img)</code>: Takes in an image, return the image with black background and detected skin foreground.<br />

<code>gesture_identifier(mirror=False)</code>: The main program running the gesture classification algorithm.<br />

<code> def ncc(img, template)</code>: Take the grayscale sub-image and template as input. Output the normalized correlation coefficient value.<br />

<code> def remove_padding(binary_imgs) </code>: Take the binary image as input, Output the de-noised and no-padding image (only blob itself in the bounding box).<br />

<code> def template_matching(imgs, templates, method='binary_ssd')</code>: Take a sequence of grayscale images and templates with a method parameter (binary_ssd or grayscale_ncc) as input. Output the best-matched the template for each of the image in the sequence along with the ssd/ncc values.<br />





<hr>
<h2>Experiments</h2>
<p>
      The experiment is completed in Python. <b>Source Code can be found  
      <a href="./code"> here</a>.</b>
    </p>
    
    <p>
      There are several templates used in our experiment, only some of them are shown in this page. <b>All templates can be found <a href="./templates"> here</a>.</b>
    </p>
    
    <table>
      <caption><h3>Templates of Hand Shapes</h3></caption>
      <tr>
        <th> Fist </th> 
        <th> Thumb Up </th>
        <th> Palm </th>
        <th> Index Finger </th>
      </tr>
      <tr>
        <td> <img class = 'img' src="./templates/left fist 1covered copy-grayscale.jpg"> </td> 
        <td> <img class = 'img' src="./templates/left thumb up copy-grayscale.jpg"> </td> 
        <td> <img class = 'img' src="./templates/right palm copy-grayscale.jpg"> </td> 
        <td> <img class = 'img' src="./templates/index finger 2-grayscale.jpg"> </td> 
      </tr> 
    </table>

<!--
<p>
Describe your experiments, including the number of tests that you
performed, and the relevant parameter values. 

</p>

<p>
Define your evaluation
metrics, e.g., detection rates, accuracy, running time. </p>
-->

<hr>

<h2> Results</h2>
<p>
<!--
List your experimental results.  Provide examples of input images and output
images. If relevant, you may provide images showing any intermediate steps.  If
your work involves videos, do not submit the videos but only links to them.
</p>
-->

    <table>
    <caption><h3>Classification</h3></caption>
    
    <tr>
    <th> Hand </th>
    <th> Fist </th>
    <th> Thumb up </th>
    <th> Thumb down </th>
    </tr>
    <tr>
      <td> <img class = 'res' src="result/hand.png"> </td> 
      <td> <img class = 'res' src="result/fist.png"> </td> 
      <td> <img class = 'res' src="result/tup.png"> </td> 
      <td> <img class = 'res' src="result/tdown.png"> </td>
    </tr> 
    <tr>
    <th> Index finger </th>
    <th> Double hands </th>
    <th> Waving </th>
    <th> Drawing </th>
    </tr>
    <tr>
      <td> <img class = 'res' src="result/index.png"> </td> 
      <td> <img class = 'res' src="result/2hands.png"> </td> 
      <td> <img class = 'res' src="result/waving.png"> </td> 
      <td> <img class = 'res' src="result/drawing.png"> </td>
    </tr> 
    </table>


    <table class = 'c'>
    <caption><h3>Confusion matrix</h3></caption>
    
    <tr class = 'c'>
    <th class = 'c'></th>
    <th class = 'c'> Hand </th>
    <th class = 'c'> Fist </th>
    <th class = 'c'> Thumb up </th>
    <th class = 'c'> Thumb down </th>
    <th class = 'c'> Index finger </th>
    </tr>
    <tr class = 'c'>
      <td class = 'c'>Hand</td> 
      <td class = 'c'>9</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>0</td>
      <td class = 'c'>0</td>  
    </tr>
    <tr class = 'c'>
      <td class = 'c'>Fist</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>7</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>2</td>
      <td class = 'c'>0</td>  
    </tr> 
    <tr class = 'c'>
      <td class = 'c'>Thump up</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>4</td> 
      <td class = 'c'>0</td>
      <td class = 'c'>2</td>  
    </tr> 
    <tr class = 'c'>
      <td class = 'c'>Thumb down</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>4</td>
      <td class = 'c'>0</td>  
    </tr>
    <tr class = 'c'>
      <td class = 'c'>Index finger</td> 
      <td class = 'c'>0</td> 
      <td class = 'c'>1</td> 
      <td class = 'c'>2</td> 
      <td class = 'c'>0</td>
      <td class = 'c'>8</td>  
    </tr>  
    </table>
<hr>
<h2> Discussion </h2>


<!--
Discuss your method and results:
<ul>
<li>What are the strengths and weaknesses of your method? </li>
<li>Do your results show that your method is generally successful or
     are there limitations? Describe what you expected to find in your
     experiments, and how that differed or was confirmed by your
     results. </li>
<li>Potential future work. How could your method be improved?   What
would you try (if you had more time) to overcome the
failures/limitations of your work?</li> 
</ul>
-->

    <p>The problem of thresholding small objects that are similar to skin color is that the skin area detected is subject to the distance from the camera to the hand. If the hand is too far away to the camera, it might not be classified as a skin blob. However, the skin detection algorithm is also a hard-coded thresholding that is very prone to lighting conditions. The skin in a darker area may not even be recognized as skin. 
    </p>
    <p>
      Our template matching implementation rotates the template and subimage for plus or minus 15 degrees, which provides a little bit of robustness to our algorithm. I think the better performing template matching algorithm will first calculate the orientation of the subimage found, then match its orientation with the orientation of the template. 
    </p>
    <p>
      The drawback of drawing classification: It is only tracking average error in x coordinate, which means if the hand is moving vertically the algorithm cannot properly classify the movement as drawing. 
    </p>
<hr>
<h2> Conclusion </h2>

<p>
<!--
Based on your discussion, what are your conclusions?  What is your
main message?-->
This project allowed us to familiarize ourselves with traditional methods of object detection using color detection. It also exposed many of the drawbacks to traditional methods, such as lighting environment, low efficiency, and sensitivity to the distortion of an image. However, it is a easy  solution that can be quickly implemented in many applications with average accuracy requirements. Further improvements of the project include vectorization of template matching and migration from Python to C++.
</p>

<hr>
<h2> Credits and Bibliography </h2>
<!--
<p>

Cite any papers or other references you consulted while developing
your solution.  Citations to papers should include the authors, the
year of publication, the title of the work, and the publication
information (e.g., book name and publisher; conference proceedings and
location; journal name, volume and pages; technical report and
institution).  

<p>
Material on the web should include the url and date of access.
</p>

<p>
Credit any joint work or discussions with your classmates. 
</p>
<hr>
-->
<p>
            CS 585 - Lab 3 Solution - Teaching Fellow Yifu Hu <br>
            Classmates: [Teammate 1], [Teammate 2]
</p>

</div>
</body>



</html>
